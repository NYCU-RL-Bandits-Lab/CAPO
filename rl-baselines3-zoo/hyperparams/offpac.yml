
atari:
  env_wrapper:
    - stable_baselines3.common.atari_wrappers.AtariWrapper
  frame_stack: 4
  policy: 'CnnPolicy'
  n_envs: 10
  batch_size: 5
  buffer_size: 300
  n_timesteps: !!float 1e7
  target_update_interval: 1
  gradient_steps: 30
  behav_update_interval: 30
  tau: 0.05
  behav_tau: 1
  learning_starts: 10000
  train_freq: (10, step)
  vf_coef: 0.25
  exploration_final_eps: 0.05
  exploration_fraction: 0.3
  exploration_initial_eps: 0.2
  max_alpha: 20
  reg_coef: 0
  ent_coef: 0
  learning_rate: !!float 9e-4
  # policy_kwargs: "dict(net_arch=[128, 128])"
  # policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))"


MinAtar/Breakout-v0:
  env_wrapper:
    - utils.wrappers.MinAtarWrapper
  n_timesteps: !!float 1e7
  policy: 'CnnPolicy'
  # policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))"
  gamma: 0.995
  # train_freq: (5, step)
  train_freq: (20, step)
  # train_freq: (1, episode)
  # learning_rate: lin_0.00043
  # learning_rate: lin_0.00053
  learning_rate: lin_0.0005
  tau: 0.05
  target_update_interval: 1
  behav_tau: 1
  batch_size: 32
  # behav_update_interval: 50
  behav_update_interval: 50
  buffer_size: 6400
  gradient_steps: 30
  max_alpha: 50
  learning_starts: 100
  n_envs: 40
  vf_coef: 1
  exploration_final_eps: 0.05
  exploration_fraction: 0.1
  exploration_initial_eps: 0.3
  ent_coef: 0
  reg_coef: 0
  share: False


MinAtar/Freeway-v0:
  env_wrapper:
    - utils.wrappers.MinAtarWrapper
  n_timesteps: !!float 1e7
  policy: 'CnnPolicy'
  # policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))"
  gamma: 0.995
  # train_freq: (5, step)
  train_freq: (20, step)
  # train_freq: (1, episode)
  # learning_rate: lin_0.00043
  # learning_rate: lin_0.00053
  learning_rate: lin_0.0005
  tau: 0.05
  target_update_interval: 1
  behav_tau: 1
  batch_size: 32
  # behav_update_interval: 50
  behav_update_interval: 50
  buffer_size: 6400
  gradient_steps: 30
  max_alpha: 50
  learning_starts: 100
  n_envs: 40
  vf_coef: 1
  exploration_final_eps: 0.05
  exploration_fraction: 0.1
  exploration_initial_eps: 0.3
  ent_coef: 0
  reg_coef: 0
  share: False


  
MinAtar/Seaquest-v0:
  env_wrapper:
    - utils.wrappers.MinAtarWrapper
  n_timesteps: !!float 1e7
  policy: 'CnnPolicy'
  # policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))"
  gamma: 0.995
  # train_freq: (5, step)
  train_freq: (20, step)
  # train_freq: (1, episode)
  # learning_rate: lin_0.00043
  # learning_rate: lin_0.00053
  learning_rate: lin_0.0005
  tau: 0.05
  target_update_interval: 1
  behav_tau: 1
  batch_size: 32
  # behav_update_interval: 50
  behav_update_interval: 50
  buffer_size: 6400
  gradient_steps: 30
  max_alpha: 50
  learning_starts: 100
  n_envs: 40
  vf_coef: 1
  exploration_final_eps: 0.05
  exploration_fraction: 0.1
  exploration_initial_eps: 0.3
  ent_coef: 0
  reg_coef: 0
  share: False


MinAtar/SpaceInvaders-v0:
  env_wrapper:
    - utils.wrappers.MinAtarWrapper
  n_timesteps: !!float 1e7
  policy: 'CnnPolicy'
  # policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))"
  gamma: 0.995
  # train_freq: (5, step)
  train_freq: (20, step)
  # train_freq: (1, episode)
  # learning_rate: lin_0.00043
  # learning_rate: lin_0.00053
  learning_rate: lin_0.0005
  tau: 0.05
  target_update_interval: 1
  behav_tau: 1
  batch_size: 32
  # behav_update_interval: 50
  behav_update_interval: 50
  buffer_size: 6400
  gradient_steps: 30
  max_alpha: 50
  learning_starts: 100
  n_envs: 40
  vf_coef: 1
  exploration_final_eps: 0.05
  exploration_fraction: 0.1
  exploration_initial_eps: 0.3
  ent_coef: 0
  reg_coef: 0
  share: False
  reg_coef: 0

MinAtar/Asterix-v0:
  env_wrapper:
    - utils.wrappers.MinAtarWrapper
  n_timesteps: !!float 1e7
  policy: 'CnnPolicy'
  # policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))"
  gamma: 0.995
  # train_freq: (5, step)
  train_freq: (20, step)
  # train_freq: (1, episode)
  # learning_rate: lin_0.00043
  # learning_rate: lin_0.00053
  learning_rate: lin_0.0005
  tau: 0.05
  target_update_interval: 1
  behav_tau: 1
  batch_size: 32
  # behav_update_interval: 50
  behav_update_interval: 50
  buffer_size: 6400
  gradient_steps: 30
  max_alpha: 50
  learning_starts: 100
  n_envs: 40
  vf_coef: 1
  exploration_final_eps: 0.05
  exploration_fraction: 0.1
  exploration_initial_eps: 0.3
  ent_coef: 0
  reg_coef: 0
  share: False



CartPole-v0:
  normalize: false
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  learning_rate: !!float 2.3e-4
  batch_size: 30
  buffer_size: 3200
  target_update_interval: 1
  tau: 0.05
  # tau: 1
  behav_tau: 1
  vf_coef: 1
  behav_update_interval: 10
  learning_starts: 100
  gamma: 0.99
  train_freq: (10, step)
  n_envs: 3
  gradient_steps: 30
  exploration_final_eps: 0.04
  exploration_fraction: 0.3
  exploration_initial_eps: 0.3
  max_alpha: 50
  reg_coef: 0
  ent_coef: 0
  policy_kwargs: "dict(net_arch=[64, 64])"
  share: False
  
  


LunarLander-v2:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.995
  train_freq: (10, step)
  # train_freq: (1, episode)
  learning_rate: lin_0.00083
  # learning_rate: !!float 1e-3
  tau: 0.05
  target_update_interval: 1
  behav_tau: 1
  batch_size: 8
  behav_update_interval: 400
  buffer_size: 6400
  gradient_steps: 30
  max_alpha: 50
  learning_starts: 10000
  n_envs: 16
  vf_coef: 0.25
  exploration_final_eps: 0.05
  exploration_fraction: 0.1
  exploration_initial_eps: 0.3
  ent_coef: 0
  reg_coef: 0
  # policy_kwargs: "dict(net_arch=[128, 128, 128])"
  # policy_kwargs: "dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])])"

MountainCar-v0:
  normalize: true
  policy: 'MlpPolicy'
  n_timesteps: !!float 3e6
  learning_rate: !!float 7e-4
  batch_size: 16
  buffer_size: 1024
  learning_starts: 1000
  gamma: 0.99
  train_freq: (10, episode)
  # train_freq: (200, step)
  tau: 0.8
  target_update_interval: 1
  behav_update_interval: 100
  n_envs: 16
  exploration_final_eps: 0.05
  exploration_fraction: 0.3
  exploration_initial_eps: 1
  gradient_steps: 1
  vf_coef: 0.5
  policy_kwargs: "dict(net_arch=[128, 128])"


Acrobot-v1:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  ent_coef: .0

# Almost tuned
Pendulum-v0:
  normalize: True
  n_envs: 8
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_7e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
LunarLanderContinuous-v2:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_7e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
MountainCarContinuous-v0:
  # env_wrapper: utils.wrappers.PlotActionWrapper
  normalize: true
  n_envs: 4
  n_steps: 100
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  ent_coef: 0.0
  use_sde: True
  sde_sample_freq: 16
  policy_kwargs: "dict(log_std_init=0.0, ortho_init=False)"

# Tuned
BipedalWalker-v3:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
BipedalWalkerHardcore-v3:
  normalize: true
  n_envs: 32
  n_timesteps: !!float 20e7
  policy: 'MlpPolicy'
  ent_coef: 0.001
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.0008
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
HalfCheetahBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  # Both works
  learning_rate: lin_0.00096
  # learning_rate: !!float 3e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False, full_std=True)"

Walker2DBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"


# Tuned
AntBulletEnv-v0:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
HopperBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned but unstable
# Not working without SDE?
ReacherBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.0008
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"
